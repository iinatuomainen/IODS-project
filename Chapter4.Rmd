---
title: "chapter4.Rmd"
output: html_document
---

```{r}
#Load data and explore the structure and the dimensions of the data.
library(MASS)
data("Boston")
str(Boston)
dim(Boston)
```

#Data Boston includes 506 observations and 14 variables. Most of the variables are numerical and one categirical variable.

```{r}
#Show a graphical overview of the data and sho summaries of the variables in the data
summary(Boston)
library(corrplot)
m <- cor(Boston)
corrplot(m, method = "circle")
```

#One of the most positive correlation may be between nitrogen oxides concentration and proportion of non-retail business acres per town. Whereas, one of the most negative relationship is between age and weighted mean of distance to five boston employment centers. 

```{r}
#Standardize the dataset and print out summaries of the scaled data.
boston_scaled <- scale(Boston)
summary(boston_scaled)
boston_scaled <- as.data.frame(boston_scaled)
```

#Means of variables are now 0.00 in every variable.

```{r}
#Create a categorial variable of the crime rate
summary(boston_scaled$crim)
bins <- quantile(boston_scaled$crim)
crime <- cut(boston_scaled$crim, breaks = bins, include.lowest = TRUE, labels = c("low", "med_low", "med_high", "high"))
```

```{r}
#Drop the olf crime rate variable from the data set
boston_scaled <- dplyr::select(boston_scaled, -crim)
boston_scaled <- data.frame(boston_scaled, crime)
```


```{r}
#Divide the dataset to train and test sets
n <- nrow(boston_scaled)
ind <- sample(n, size = n*0.8)
train <- boston_scaled[ind, ]
test <- boston_scaled
```

```{r}
#Fit the linear discriminant analysis on the train set.
lda.fit <- lda(crime ~ ., data=train)
```

```{r}
#Draw the LDA plot
classes <- as.numeric(train$crime)
plot(lda.fit, dimen = 2, col = classes, pch = classes)

lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}

plot(lda.fit, dimen = 2, col = classes, pch = classes) 
lda.arrows(lda.fit, myscale = 1)
```

```{r}
#Save the crime categories from the test set and then remove the categorical crime variable from the dataset.
crime_cat <- test$crime
test <- dplyr::select(test, -crime)
summary(test)
```

```{r}
#Predict the classes with the LDA model on the test data. Cross tabulate the results.
lda.pred <- predict(lda.fit, newdata = test)
table(correct = crime_cat, predicted = lda.pred$class)
```

```{r}
#Reload the Boston dataset and standardize the dataset. Calculate the distances between the observations. Run k-means and investigates the optimal number of clusters. Visualize the clustes.
data('Boston')
Boston2 <- scale(Boston)
dist_eu <- dist(Boston2)
summary(dist_eu)

set.seed(123)
k_max <- 10
twcss <- sapply(1:k_max, function(k){kmeans(Boston2, k)$tot.withinss})

library(ggplot2)
qplot(x=1:k_max, y=twcss, geom = 'line')

km <- kmeans(Boston2, centers = 2)
pairs(Boston2, col= km$cluster)
```

#In the qplot picture, the twcss dramatically drops in number 2, therefore 2 is optimal of clusters



